
# World Cup Event Detection using Deep Neural Networks

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-1.8%2B-red)
![License](https://img.shields.io/badge/License-MIT-green)

## Overview

This project classifies and predicts event types from tweets related to the FIFA World Cup using Deep Neural Networks (DNN). The pipeline involves processing raw tweets, embedding them as vectors, and training a DNN model for classification and prediction.

## Features

- **Text Embedding**: Uses pre-trained GloVe embeddings to convert text into vector representations.
- **Hyperparameter Optimization**: Fine-tunes the DNN model using K-fold cross-validation.
- **Event Prediction**: Classifies tweets into event categories using a trained DNN.
- **Analysis**: Provides insights with visualizations of performance metrics and feature correlations.

---

## Table of Contents

1. [Getting Started](#getting-started)
2. [Directory Structure](#directory-structure)
3. [Workflow](#workflow)
4. [Files](#files)
5. [Requirements](#requirements)
6. [Execution](#execution)
7. [Contributors](#contributors)
8. [License](#license)

---

## Getting Started

Clone the repository to your local machine:

```bash
git clone https://github.com/98kgb/world-cup-event-detection.git
cd world-cup-event-detection
```

Prepare the following directories before running the scripts:

- `train_tweets/`: Raw training datasets in `.csv` format.
- `eval_tweets/`: Evaluation datasets in `.csv` format.
- `embedded_dataset/`, `model/`, and `predictions/`: These will be populated during execution.

---

## Directory Structure

```plaintext
world-cup-event-detection/
│
├── embedded_dataset/       # Contains embedded feature vectors
├── model/
│   ├── optimization/       # Hyperparameter optimization results
│   └── info/               # Model training logs and parameters
├── train_tweets/           # Raw training tweet datasets
├── eval_tweets/            # New tweet datasets for evaluation
├── predictions/            # Evaluation results
└── figures/                # Visualizations generated by Analysis.py
```

---

## Workflow

### 1. **Embedding Tweets**
   - Script: `Embedding.py`
   - Description: Processes and embeds tweets using GloVe embeddings. Outputs embedded vectors to `embedded_dataset/`.

### 2. **Hyperparameter Optimization**
   - Script: `DNN_optimizer.py`
   - Description: Fine-tunes the DNN model using K-fold cross-validation. Results are saved in `model/optimization/`.

### 3. **Model Training**
   - Script: `DNN_train.py`
   - Description: Trains the DNN model using the best hyperparameters. The trained model is saved in `model/`.

### 4. **Evaluation**
   - Script: `Final_prediction.py`
   - Description: Evaluates the trained model on new datasets and saves predictions in `predictions/`.

### 5. **Analysis**
   - Script: `Analysis.py`
   - Description: Generates visualizations and reports on keyword distribution, correlations, and model performance.

---

## Files

| File Name            | Description                                                                 |
|----------------------|-----------------------------------------------------------------------------|
| `Embedding.py`       | Embeds tweets using GloVe embeddings and saves vectors.                   |
| `DNN_optimizer.py`   | Optimizes DNN hyperparameters using K-fold cross-validation.              |
| `DNN_classifier.py`  | Trains the DNN model with early stopping.                                 |
| `Final_prediction.py`| Evaluates the trained model on new datasets.                              |
| `Analysis.py`        | Visualizes keyword distribution, correlations, and performance metrics.   |

---

## Requirements

- Python 3.8+
- Libraries:
  - PyTorch
  - Gensim
  - Scikit-learn
  - NLTK
  - Matplotlib
  - Seaborn
  - Pandas
  - NumPy
- Pre-trained Model: GloVe Twitter embeddings (200-dimensional)

Install dependencies using `pip`:

```bash
pip install -r requirements.txt
```

---

## Execution

### Step 1: Embedding
```bash
python Embedding.py
```

### Step 2: Hyperparameter Optimization
```bash
python DNN_optimizer.py
```

### Step 3: Training
```bash
python DNN_classifier.py
```

### Step 4: Evaluation
```bash
python Final_prediction.py
```

### Step 5: Analysis
```bash
python Analysis.py
```

---

## Contributors

- **Gibaek kim**: Author of all the files and contributor to the project framework.

---

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

---

